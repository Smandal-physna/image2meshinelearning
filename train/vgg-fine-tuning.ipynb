{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.utils import class_weight\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D\n",
    "# from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "\n",
    "top_model_weights_path = '128_128_2_2_4classes.h5'#bottleneck_fc_model_multi_40epoch_6class_2lyr.h5'\n",
    "save_weights = 'fin_ep30_bt64_bed_book_ch_desk_sof_tb_4class.h5'\n",
    "train_data_dir = 'pix3d-ML/train'\n",
    "validation_data_dir = 'pix3d-ML/validation'\n",
    "test_data_dir = 'pix3d-ML/test'\n",
    "botfet_train = 'bottleneck_features_train.npy'\n",
    "botfet_validation = 'bottleneck_features_validation.npy'\n",
    "num_classes = 4\n",
    "# t = [1049,914,1097,1245,1056,1743]\n",
    "# z = [249,191,254,244,244,354] #sum must be div by 16\n",
    "t = [920,1097,1056,1743]#1049,1245,\n",
    "z = [192,254,244,350] #sum must be div by 16 #249,244,\n",
    "nb_train_samples = sum(t)\n",
    "nb_validation_samples = sum(z)\n",
    "epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4816 images belonging to 4 classes.\n",
      "Found 1040 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255)\n",
    "#     shear_range=0.3,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.3541666666666667, 1: 1.0236220472440944, 2: 1.0655737704918034, 3: 0.7428571428571429}\n"
     ]
    }
   ],
   "source": [
    "#Take care on unbalanced class\n",
    "# y_train = np.load(open('label_train.npy','rb'))\n",
    "y_train = np.array([0]*z[0] + [1]*z[1] + [2]*z[2] + [3]*z[3])\n",
    "# print(y_train)\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "\n",
    "cl = {}\n",
    "for i in range(len(class_weights)):\n",
    "    cl[i] = class_weights[i]\n",
    "print(cl)\n",
    "# cl={0:1.12869399,1:1.29540481,2:1.0793072,3:0.95100402,4:1.12121212,5:0.67928858}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "Model saved.\n",
      "(7, 7, 512) (None, 7, 7, 512)\n",
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n",
      "75/75 [==============================] - 15s 203ms/step - loss: 0.2122 - accuracy: 0.9329 - val_loss: 0.1415 - val_accuracy: 0.9518\n",
      "Epoch 2/30\n",
      "75/75 [==============================] - 14s 181ms/step - loss: 0.1581 - accuracy: 0.9455 - val_loss: 0.1090 - val_accuracy: 0.9652\n",
      "Epoch 3/30\n",
      "75/75 [==============================] - 14s 183ms/step - loss: 0.1574 - accuracy: 0.9478 - val_loss: 0.0983 - val_accuracy: 0.9703\n",
      "Epoch 4/30\n",
      "75/75 [==============================] - 13s 179ms/step - loss: 0.1516 - accuracy: 0.9499 - val_loss: 0.0927 - val_accuracy: 0.9693\n",
      "Epoch 5/30\n",
      "75/75 [==============================] - 13s 178ms/step - loss: 0.1290 - accuracy: 0.9583 - val_loss: 0.0858 - val_accuracy: 0.9775\n",
      "Epoch 6/30\n",
      "75/75 [==============================] - 13s 178ms/step - loss: 0.1215 - accuracy: 0.9571 - val_loss: 0.0946 - val_accuracy: 0.9682\n",
      "Epoch 7/30\n",
      "75/75 [==============================] - 14s 182ms/step - loss: 0.1224 - accuracy: 0.9596 - val_loss: 0.0805 - val_accuracy: 0.9795\n",
      "Epoch 8/30\n",
      "75/75 [==============================] - 14s 183ms/step - loss: 0.1198 - accuracy: 0.9581 - val_loss: 0.0872 - val_accuracy: 0.9723\n",
      "Epoch 9/30\n",
      "75/75 [==============================] - 14s 185ms/step - loss: 0.1224 - accuracy: 0.9596 - val_loss: 0.0752 - val_accuracy: 0.9754\n",
      "Epoch 10/30\n",
      "75/75 [==============================] - 14s 185ms/step - loss: 0.1184 - accuracy: 0.9604 - val_loss: 0.0676 - val_accuracy: 0.9764\n",
      "Epoch 11/30\n",
      "75/75 [==============================] - 14s 186ms/step - loss: 0.1123 - accuracy: 0.9642 - val_loss: 0.0807 - val_accuracy: 0.9775\n",
      "Epoch 12/30\n",
      "75/75 [==============================] - 15s 194ms/step - loss: 0.1153 - accuracy: 0.9588 - val_loss: 0.0644 - val_accuracy: 0.9826\n",
      "Epoch 13/30\n",
      "75/75 [==============================] - 14s 193ms/step - loss: 0.1125 - accuracy: 0.9632 - val_loss: 0.0749 - val_accuracy: 0.9764\n",
      "Epoch 14/30\n",
      "75/75 [==============================] - 14s 193ms/step - loss: 0.1178 - accuracy: 0.9598 - val_loss: 0.0707 - val_accuracy: 0.9795\n",
      "Epoch 15/30\n",
      "75/75 [==============================] - 15s 197ms/step - loss: 0.1049 - accuracy: 0.9661 - val_loss: 0.0732 - val_accuracy: 0.9746\n",
      "Epoch 16/30\n",
      "75/75 [==============================] - 14s 191ms/step - loss: 0.1002 - accuracy: 0.9659 - val_loss: 0.0640 - val_accuracy: 0.9764\n",
      "Epoch 17/30\n",
      "75/75 [==============================] - 15s 195ms/step - loss: 0.1087 - accuracy: 0.9628 - val_loss: 0.0703 - val_accuracy: 0.9785\n",
      "Epoch 18/30\n",
      "75/75 [==============================] - 15s 195ms/step - loss: 0.1051 - accuracy: 0.9668 - val_loss: 0.0848 - val_accuracy: 0.9713\n",
      "Epoch 19/30\n",
      "75/75 [==============================] - 15s 197ms/step - loss: 0.1176 - accuracy: 0.9606 - val_loss: 0.0666 - val_accuracy: 0.9785\n",
      "Epoch 20/30\n",
      "75/75 [==============================] - 15s 196ms/step - loss: 0.1068 - accuracy: 0.9628 - val_loss: 0.0615 - val_accuracy: 0.9785\n",
      "Epoch 21/30\n",
      "75/75 [==============================] - 15s 196ms/step - loss: 0.1090 - accuracy: 0.9659 - val_loss: 0.0745 - val_accuracy: 0.9775\n",
      "Epoch 22/30\n",
      "75/75 [==============================] - 15s 197ms/step - loss: 0.1167 - accuracy: 0.9615 - val_loss: 0.0661 - val_accuracy: 0.9795\n",
      "Epoch 23/30\n",
      "75/75 [==============================] - 15s 198ms/step - loss: 0.1130 - accuracy: 0.9636 - val_loss: 0.0627 - val_accuracy: 0.9795\n",
      "Epoch 24/30\n",
      "75/75 [==============================] - 14s 188ms/step - loss: 0.1063 - accuracy: 0.9659 - val_loss: 0.0657 - val_accuracy: 0.9744\n",
      "Epoch 25/30\n",
      "75/75 [==============================] - 14s 189ms/step - loss: 0.0956 - accuracy: 0.9686 - val_loss: 0.0592 - val_accuracy: 0.9816\n",
      "Epoch 26/30\n",
      "75/75 [==============================] - 14s 192ms/step - loss: 0.0953 - accuracy: 0.9689 - val_loss: 0.0664 - val_accuracy: 0.9785\n",
      "Epoch 27/30\n",
      "75/75 [==============================] - 15s 194ms/step - loss: 0.0970 - accuracy: 0.9680 - val_loss: 0.0622 - val_accuracy: 0.9785\n",
      "Epoch 28/30\n",
      "75/75 [==============================] - 14s 188ms/step - loss: 0.0997 - accuracy: 0.9680 - val_loss: 0.0480 - val_accuracy: 0.9836\n",
      "Epoch 29/30\n",
      "75/75 [==============================] - 14s 183ms/step - loss: 0.0943 - accuracy: 0.9703 - val_loss: 0.0613 - val_accuracy: 0.9785\n",
      "Epoch 30/30\n",
      "75/75 [==============================] - 14s 181ms/step - loss: 0.1093 - accuracy: 0.9653 - val_loss: 0.0724 - val_accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "# server = tf.train.Server.create_local_server()\n",
    "# sess = tf.Session(server.target)\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.set_session(sess)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(weights='imagenet', include_top=False,input_shape = (224,224,3))\n",
    "    print('Model saved.')\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    print(model.output_shape[1:],model.output_shape)\n",
    "#     top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "#     top_model.add(Dense(256, activation='relu'))\n",
    "#     top_model.add(Dropout(0.5))\n",
    "#     top_model.add(Dense(num_classes, activation='softmax'))\n",
    "    top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    top_model.add(Dense(128, activation='relu'))\n",
    "    top_model.add(Dropout(0.2))\n",
    "    top_model.add(Dense(128, activation='relu'))\n",
    "    top_model.add(Dropout(0.3))\n",
    "    top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "    top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "    mb = Sequential()\n",
    "    mb.add(model)\n",
    "    mb.add(top_model)\n",
    "    # model.add(top_model)\n",
    "    model = mb\n",
    "#     for layer in model.layers[:25]:\n",
    "#         layer.trainable = False\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-5, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    class_weight=cl)\n",
    "\n",
    "model.save_weights(save_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_load():\n",
    "    model = applications.VGG16(weights='imagenet', include_top=False,input_shape = (224,224,3))\n",
    "    print('Model saved.')\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    print(model.output_shape[1:],model.output_shape)\n",
    "    top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # note that it is necessary to start with a fully-trained\n",
    "    # classifier, including the top classifier,\n",
    "    # in order to successfully do fine-tuning\n",
    "    top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "    # add the model on top of the convolutional base\n",
    "    mb = Sequential()\n",
    "    mb.add(model)\n",
    "    mb.add(top_model)\n",
    "    # model.add(top_model)\n",
    "#     model = mb\n",
    "    mb.load_weights('fin_ep50_bt64_bed_book_ch_desk_sof_tb.h5')\n",
    "    return mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bookcase': 0, 'chair': 1, 'sofa': 2, 'table': 3}\n"
     ]
    }
   ],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image,ImageFilter\n",
    "\n",
    "# model = only_load()\n",
    "\n",
    "pos = [0]*num_classes #pos,neg: match and mismatch for each class\n",
    "neg = [0]*num_classes\n",
    "comp = [pos*num_classes]\n",
    "#what is the highest wrong class for each class\n",
    "mapp = {'bookcase': 0, 'chair': 1, 'sofa': 2, 'table': 3}\n",
    "for r,d,f in os.walk(test_data_dir):\n",
    "    for files in f:\n",
    "        rr = r.split('/')\n",
    "        idr = mapp[rr[2]]\n",
    "        \n",
    "        image2 = np.asarray(Image.open(r+'/'+files))\n",
    "        image2 = image2[:,:,0:3]\n",
    "#             print(image2.shape)\n",
    "        image2 = np.reshape(image2,[1,224,224,3])\n",
    "        classes = model.predict(image2)[0]\n",
    "#             classes = classes[0]\n",
    "#             print(classes)\n",
    "        if sorted(classes)[-2]!=0:\n",
    "            sr = [sorted(classes)[-2],sorted(classes)[-1]]\n",
    "        else:\n",
    "            sr = [sorted(classes)[-1]]\n",
    "#         print('cls:',classes,sr,idr,rr)\n",
    "        \n",
    "        if classes[idr] in sr:\n",
    "            pos[idr]+=1\n",
    "            if classes[idr]!=sr[-1]:\n",
    "#                 print(classes,np.where(classes==sr[-1])[0][0])\n",
    "                comp[idr][np.where(classes==sr[-1])[0][0]]+=1\n",
    "        else:\n",
    "            neg[idr]+=1\n",
    "                \n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 82, 312, 92, 0, 0] [31, 22, 81, 20, 0, 0] [[0, 3, 0, 1, 0, 0], [0, 0, 3, 5, 0, 0], [0, 32, 0, 9, 0, 0], [0, 8, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(pos,neg,comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34146341463414637\n",
      "0.6231884057971014\n",
      "0.6923076923076923\n",
      "0.22580645161290322\n",
      "0.72264631043257\n",
      "0.8839285714285714\n"
     ]
    }
   ],
   "source": [
    "for en,item in enumerate(pos):\n",
    "    print(item/(item+neg[en]))\n",
    "    #128,128,2,2 all layer unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7753623188405797\n",
      "0.7884615384615384\n",
      "0.7938931297709924\n",
      "0.8214285714285714\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3db82cfe6682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for en,item in enumerate(pos):\n",
    "    print(item/(item+neg[en]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4146341463414634\n",
      "0.5434782608695652\n",
      "0.46153846153846156\n",
      "0.3118279569892473\n",
      "0.8193384223918575\n",
      "0.8660714285714286\n"
     ]
    }
   ],
   "source": [
    "for en,item in enumerate(pos):\n",
    "    print(item/(item+neg[en]))\n",
    "    # 128 128 2 2 layer frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6676829268292683\n",
      "0.5\n",
      "0.46153846153846156\n",
      "0.17204301075268819\n",
      "0.5928753180661578\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "for en,item in enumerate(pos):\n",
    "    print(item/(item+neg[en]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3170731707317073\n",
      "0.6086956521739131\n",
      "0.7019230769230769\n",
      "0.22580645161290322\n",
      "0.6666666666666666\n",
      "0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "for en,item in enumerate(pos):\n",
    "    print(item/(item+neg[en]))\n",
    "    #128,128,2,2 all layer unfreeze e-5,40epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
